{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time keypoint detection using webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "import torch\n",
    "from io import BytesIO as StringIO\n",
    "import PIL.Image\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout3): Dropout(p=0.3, inplace=False)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout4): Dropout(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=43264, out_features=1000, bias=True)\n",
       "  (bn5): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout5): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (bn6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout6): Dropout(p=0.6, inplace=False)\n",
       "  (fc3): Linear(in_features=1000, out_features=136, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import Net\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "net = Net()\n",
    "\n",
    "model_dir = \"models\"\n",
    "model_name = \"keypoints_model.pt\"\n",
    "\n",
    "net.load_state_dict(torch.load(os.path.join(model_dir, model_name)))\n",
    "\n",
    "# print out net\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a haar cascade classifier for detecting frontal faces\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    \"detector_architectures/haarcascade_frontalface_default.xml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(img):\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(img, 1.2, 2)\n",
    "\n",
    "    # make a copy of the original image to plot detections on\n",
    "    image_with_detections = img.copy()\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # draw a rectangle around each detected face\n",
    "        # we also need to change the width of the rectangle drawn depending on image resolution\n",
    "        cv2.rectangle(image_with_detections, (x, y), (x + w, y + h), (255, 0, 0), 3)\n",
    "\n",
    "    return image_with_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'jpeg' instead of 'png' (~5 times faster)\n",
    "def showarray(a, fmt=\"jpeg\"):\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame(cam):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # flip image for natural viewing\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keypoints(img, scale):\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(img, 1.2, 2)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return img\n",
    "\n",
    "    image_copy = np.copy(img)\n",
    "\n",
    "    # loop over the detected faces from the haar cascade\n",
    "    for (x, y, w, h) in faces:\n",
    "\n",
    "        # don't scale ouside of the frame!\n",
    "        if (y - scale) < 0 and (x - scale) < 0:\n",
    "            if (y - scale) < (x - scale):\n",
    "                scale += y - scale\n",
    "            else:\n",
    "                scale += x - scale\n",
    "        elif (y - scale) < 0:\n",
    "            scale += y - scale\n",
    "        elif (x - scale) < 0:\n",
    "            scale += x - scale\n",
    "\n",
    "        # Select the region of interest that is the face in the image\n",
    "        roi = image_copy[y - scale : y + h + scale, x - scale : x + w + scale]\n",
    "\n",
    "        roi_color = np.copy(roi)\n",
    "\n",
    "        # Convert the face region from RGB to grayscale\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)\n",
    "        # Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]\n",
    "        roi = roi / 255.0\n",
    "        # Rescale the detected face to be the expected square size for the CNN (224x224, suggested)\n",
    "\n",
    "        h, w = roi.shape\n",
    "\n",
    "        shape_before_resize = roi.shape\n",
    "\n",
    "        roi = cv2.resize(roi, (img_size, img_size))\n",
    "\n",
    "        shape_after_resize = roi.shape\n",
    "\n",
    "        # how much the image was scaled with\n",
    "        # will use to resize and fit to the webcam image\n",
    "        scaling_factor = shape_before_resize[0] / shape_after_resize[0]\n",
    "\n",
    "        roi_color = cv2.resize(roi_color, (img_size, img_size))\n",
    "        # Make copy for displaying keypoint over\n",
    "        roi_copy = np.copy(roi)\n",
    "\n",
    "        # Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
    "\n",
    "        # if image has no grayscale color channel, add one\n",
    "        if len(roi.shape) == 2:\n",
    "            # add that third color dim\n",
    "            roi = roi.reshape(roi.shape[0], roi.shape[1], 1)\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        roi = roi.transpose((2, 0, 1))\n",
    "\n",
    "        roi = torch.from_numpy(roi)\n",
    "        roi = roi.type(torch.FloatTensor)\n",
    "\n",
    "        roi.unsqueeze_(0)\n",
    "        # Make facial keypoint predictions using the loaded, trained network\n",
    "        # perform a forward pass to get the predicted facial keypoints\n",
    "        output_pts = net(roi)\n",
    "        # reshape to size x 68 x 2 pts\n",
    "        output_pts = output_pts.view(68, -1)\n",
    "\n",
    "        # undo normalization of keypoints\n",
    "        output_pts = output_pts.detach().numpy()\n",
    "        output_pts = output_pts * (roi_copy.shape[0] / 4) + roi_copy.shape[0] / 2\n",
    "        for pts in output_pts:\n",
    "            pts[0] = x - scale + pts[0] * scaling_factor\n",
    "            pts[1] = y - scale + pts[1] * scaling_factor\n",
    "\n",
    "        # Draw mask\n",
    "        color = (0, 255, 0)\n",
    "        for i in range(len(output_pts)):\n",
    "\n",
    "            if i != 16 and i != 21 and i != 26 and i != 30 and i != 35 and i < 68:\n",
    "                pt1 = (int(output_pts[i][0]), int(output_pts[i][1]))\n",
    "\n",
    "                if i == 17:\n",
    "                    # left eyebrow\n",
    "                    color = (0, 100, 0)\n",
    "                elif i == 22:\n",
    "                    # right eyebrow\n",
    "                    color = (0, 100, 0)\n",
    "                elif i == 27:\n",
    "                    # nose stem\n",
    "                    color = (255, 255, 0)\n",
    "                elif i == 31:\n",
    "                    # nose tip\n",
    "                    color = (255, 255, 0)\n",
    "                elif i == 36:\n",
    "                    # left eye\n",
    "                    color = (0, 250, 154)\n",
    "                elif i == 42:\n",
    "                    # right eye\n",
    "                    color = (0, 250, 154)\n",
    "                elif i == 48:\n",
    "                    # lips\n",
    "                    color = (255, 20, 147)\n",
    "\n",
    "                if i == 41:\n",
    "                    pt2 = (int(output_pts[36][0]), int(output_pts[36][1]))\n",
    "                elif i == 47:\n",
    "                    pt2 = (int(output_pts[42][0]), int(output_pts[42][1]))\n",
    "                elif i == 67:\n",
    "                    pt2 = (int(output_pts[60][0]), int(output_pts[60][1]))\n",
    "                else:\n",
    "                    pt2 = (int(output_pts[i + 1][0]), int(output_pts[i + 1][1]))\n",
    "\n",
    "                cv2.line(image_copy, pt1, pt2, color, thickness=5, lineType=8, shift=0)\n",
    "\n",
    "        return image_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_raw_keypoints(img, scale):\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(img, 1.2, 2)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return img\n",
    "\n",
    "    else:\n",
    "        image_copy = np.copy(img)\n",
    "\n",
    "        # loop over the detected faces from the haar cascade\n",
    "        for i, (x, y, w, h) in enumerate(faces):\n",
    "            cv2.rectangle(image_copy, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "\n",
    "            # Select the region of interest that is the face in the image\n",
    "            # roi = image_copy[y:y+h, x:x+w]\n",
    "            padding = 20\n",
    "            roi = image_copy[\n",
    "                y - padding : y + h + padding, x - padding : x + w + padding\n",
    "            ]\n",
    "\n",
    "            shape_before_resize = roi.shape\n",
    "\n",
    "            # Convert the face region from RGB to grayscale\n",
    "            roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "            # Normalize the grayscale image so that its color range\n",
    "            # falls in [0,1] instead of [0,255]\n",
    "            roi = roi / 255.0\n",
    "\n",
    "            # Rescale the detected face to be the expected square size for the CNN (224x224, suggested)\n",
    "            roi = cv2.resize(roi, (224, 224))\n",
    "\n",
    "            shape_after_resize = roi.shape\n",
    "\n",
    "            # Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
    "            torch_roi = roi.reshape(1, roi.shape[0], roi.shape[1], 1)\n",
    "            # (batch_size, channel, H, W)\n",
    "            torch_roi = torch_roi.transpose((0, 3, 1, 2))\n",
    "\n",
    "            # Make facial keypoint predictions using the loaded, trained network\n",
    "            # wrap each face region in a Variable and\n",
    "            # perform a forward pass to get the predicted facial keypoints\n",
    "            torch_roi = torch.from_numpy(torch_roi)\n",
    "\n",
    "            # convert images to FloatTensors\n",
    "            torch_roi = torch_roi.type(torch.FloatTensor)\n",
    "\n",
    "            # Make facial keypoint predictions using the loaded, trained network\n",
    "            output_pts = net(torch_roi)\n",
    "\n",
    "            # Display each detected face and the corresponding keypoints\n",
    "\n",
    "            # un-transform the predicted key_pts data\n",
    "            predicted_key_pts = output_pts.data\n",
    "            predicted_key_pts = predicted_key_pts.numpy()\n",
    "\n",
    "            # reshape to 68 x 2 pts\n",
    "            predicted_key_pts = predicted_key_pts[0].reshape((68, 2))\n",
    "\n",
    "            # undo normalization of keypoints\n",
    "            output_pts = predicted_key_pts * 50.0 + 100.0\n",
    "\n",
    "            # how much the image was scaled with\n",
    "            # will use to resize and fit to the webcam image\n",
    "            scaling_factor_width = shape_before_resize[0] / shape_after_resize[0]\n",
    "            scaling_factor_height = shape_before_resize[1] / shape_after_resize[1]\n",
    "\n",
    "            for pts in output_pts:\n",
    "                pts[0] = x - scale + pts[0] * scaling_factor_width\n",
    "                pts[1] = y - scale + pts[1] * scaling_factor_height\n",
    "\n",
    "            for item in output_pts:\n",
    "                cv2.drawMarker(\n",
    "                    image_copy,\n",
    "                    (int(item[0]), int(item[1])),\n",
    "                    (255, 0, 0),\n",
    "                    markerSize=5,\n",
    "                    markerType=cv2.MARKER_CROSS,\n",
    "                )\n",
    "\n",
    "            return image_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw funny shapes on detected keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "save_video = False\n",
    "\n",
    "if save_video:\n",
    "    # https://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/\n",
    "    # Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    # Define the codec and create VideoWriter object.The output is stored in 'outpput.avi' file.\n",
    "    out = cv2.VideoWriter(\n",
    "        \"output.avi\",\n",
    "        cv2.VideoWriter_fourcc(\"M\", \"J\", \"P\", \"G\"),\n",
    "        4,\n",
    "        (frame_width, frame_height),\n",
    "    )\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # start_time = time.time()\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        frame = get_frame(cap)\n",
    "\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # to display the image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        frame = detect_keypoints(frame, 30)\n",
    "\n",
    "        # write to video file\n",
    "        if save_video:\n",
    "            out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        showarray(frame)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # print(\"FPS: \", 1.0 / (time.time() - start_time)) # FPS = 1 / time to process loop\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    cap.release()\n",
    "    if save_video:\n",
    "        out.release()\n",
    "    print(\"Stream stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw raw keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "save_video = True\n",
    "\n",
    "if save_video:\n",
    "    # https://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/\n",
    "    # Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    # Define the codec and create VideoWriter object.The output is stored in 'outpput.avi' file.\n",
    "    out = cv2.VideoWriter(\n",
    "        \"output.avi\",\n",
    "        cv2.VideoWriter_fourcc(\"M\", \"J\", \"P\", \"G\"),\n",
    "        4,\n",
    "        (frame_width, frame_height),\n",
    "    )\n",
    "try:\n",
    "    while True:\n",
    "        # start_time = time.time()\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        frame = get_frame(cap)\n",
    "\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # to display the image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        frame = draw_raw_keypoints(frame, 30)\n",
    "\n",
    "        # write to video file\n",
    "        if save_video:\n",
    "            out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        showarray(frame)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        # print(\"FPS: \", 1.0 / (time.time() - start_time)) # FPS = 1 / time to process loop\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    cap.release()\n",
    "    if save_video:\n",
    "        out.release()\n",
    "    print(\"Stream stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wabcam code is based on the [displaying webcam video in IPython notebook](https://github.com/ktaletsk/NCCV/blob/master/Realtime_video_ipython.ipynb) project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 480\n"
     ]
    }
   ],
   "source": [
    "print(frame_width, frame_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
