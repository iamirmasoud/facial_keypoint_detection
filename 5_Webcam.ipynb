{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the required modules for webacm\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import PIL.Image\n",
    "from io import BytesIO as StringIO\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/resnet18_gray_epochs64_bs64_vloss0.00414.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a565848f6904>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'resnet18_gray_epochs64_bs64_vloss0.00414.pt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# print out net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\cv_course\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/resnet18_gray_epochs64_bs64_vloss0.00414.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import NaimishNet, resnet18_grayscale\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "# net = NaimishNet(image_size=img_size, kernels=[5, 5, 5, 5], use_maxp=False)\n",
    "net = resnet18_grayscale()\n",
    "\n",
    "model_dir = \"saved_models/\"\n",
    "model_name = \"resnet18_gray_epochs64_bs64_vloss0.00414.pt\"\n",
    "\n",
    "net.load_state_dict(torch.load(model_dir + model_name))\n",
    "\n",
    "# print out net\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in a haar cascade classifier for detecting frontal faces\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    \"detector_architectures/haarcascade_frontalface_default.xml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(img):\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(img, 1.2, 2)\n",
    "\n",
    "    # make a copy of the original image to plot detections on\n",
    "    image_with_detections = img.copy()\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # draw a rectangle around each detected face\n",
    "        # you may also need to change the width of the rectangle drawn depending on image resolution\n",
    "        cv2.rectangle(image_with_detections, (x, y), (x + w, y + h), (255, 0, 0), 3)\n",
    "\n",
    "    return image_with_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'jpeg' instead of 'png' (~5 times faster)\n",
    "def showarray(a, fmt=\"jpeg\"):\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame(cam):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # flip image for natural viewing\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keypoints(img, scale):\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(img, 1.2, 2)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return img\n",
    "\n",
    "    image_copy = np.copy(img)\n",
    "\n",
    "    # loop over the detected faces from your haar cascade\n",
    "    for (x, y, w, h) in faces:\n",
    "\n",
    "        # don't scale ouside of the frame!\n",
    "        if (y - scale) < 0 and (x - scale) < 0:\n",
    "            if (y - scale) < (x - scale):\n",
    "                scale += y - scale\n",
    "            else:\n",
    "                scale += x - scale\n",
    "        elif (y - scale) < 0:\n",
    "            scale += y - scale\n",
    "        elif (x - scale) < 0:\n",
    "            scale += x - scale\n",
    "\n",
    "        # Select the region of interest that is the face in the image\n",
    "        roi = image_copy[y - scale : y + h + scale, x - scale : x + w + scale]\n",
    "\n",
    "        roi_color = np.copy(roi)\n",
    "\n",
    "        ## Convert the face region from RGB to grayscale\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)\n",
    "        ## Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]\n",
    "        roi = roi / 255.0\n",
    "        ## Rescale the detected face to be the expected square size for your CNN (224x224, suggested)\n",
    "\n",
    "        h, w = roi.shape\n",
    "\n",
    "        shape_before_resize = roi.shape\n",
    "\n",
    "        roi = cv2.resize(roi, (img_size, img_size))\n",
    "\n",
    "        shape_after_resize = roi.shape\n",
    "\n",
    "        # how much the image was scaled with\n",
    "        # will use to resize and fit to the webcam image\n",
    "        scaling_factor = shape_before_resize[0] / shape_after_resize[0]\n",
    "\n",
    "        roi_color = cv2.resize(roi_color, (img_size, img_size))\n",
    "        # Make copy for displaying keypoint over\n",
    "        roi_copy = np.copy(roi)\n",
    "\n",
    "        ## Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
    "\n",
    "        # if image has no grayscale color channel, add one\n",
    "        if len(roi.shape) == 2:\n",
    "            # add that third color dim\n",
    "            roi = roi.reshape(roi.shape[0], roi.shape[1], 1)\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        roi = roi.transpose((2, 0, 1))\n",
    "\n",
    "        roi = torch.from_numpy(roi)\n",
    "        roi = roi.type(torch.FloatTensor)\n",
    "\n",
    "        roi.unsqueeze_(0)\n",
    "        ## Make facial keypoint predictions using your loaded, trained network\n",
    "        ## perform a forward pass to get the predicted facial keypoints\n",
    "\n",
    "        # forward pass to get net output\n",
    "        output_pts = net(roi)\n",
    "        # reshape to size x 68 x 2 pts\n",
    "        output_pts = output_pts.view(68, -1)\n",
    "\n",
    "        # undo normalization of keypoints\n",
    "        output_pts = output_pts.detach().numpy()\n",
    "        output_pts = output_pts * (roi_copy.shape[0] / 4) + roi_copy.shape[0] / 2\n",
    "        for pts in output_pts:\n",
    "            pts[0] = x - scale + pts[0] * scaling_factor\n",
    "            pts[1] = y - scale + pts[1] * scaling_factor\n",
    "\n",
    "        ## Draw mask\n",
    "\n",
    "        color = (0, 255, 0)\n",
    "\n",
    "        for i in range(len(output_pts)):\n",
    "            if i != 16 and i != 21 and i != 26 and i != 30 and i != 35 and i < 68:\n",
    "                pt1 = (output_pts[i][0], output_pts[i][1])\n",
    "\n",
    "                if i == 17:\n",
    "                    # left eyebrow\n",
    "                    color = (0, 100, 0)\n",
    "                elif i == 22:\n",
    "                    # right eyebrow\n",
    "                    color = (0, 100, 0)\n",
    "                elif i == 27:\n",
    "                    # nose stem\n",
    "                    color = (255, 255, 0)\n",
    "                elif i == 31:\n",
    "                    # nose tip\n",
    "                    color = (255, 255, 0)\n",
    "                elif i == 36:\n",
    "                    # left eye\n",
    "                    color = (0, 250, 154)\n",
    "                elif i == 42:\n",
    "                    # right eye\n",
    "                    color = (0, 250, 154)\n",
    "                elif i == 48:\n",
    "                    # lips\n",
    "                    color = (255, 20, 147)\n",
    "\n",
    "                if i == 41:\n",
    "                    pt2 = (output_pts[36][0], output_pts[36][1])\n",
    "                elif i == 47:\n",
    "                    pt2 = (output_pts[42][0], output_pts[42][1])\n",
    "                elif i == 67:\n",
    "                    pt2 = (output_pts[60][0], output_pts[60][1])\n",
    "                else:\n",
    "                    pt2 = (output_pts[i + 1][0], output_pts[i + 1][1])\n",
    "\n",
    "                cv2.line(image_copy, pt1, pt2, color, thickness=5, lineType=8, shift=0)\n",
    "\n",
    "        return image_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "save_video = True\n",
    "\n",
    "if save_video:\n",
    "    # https://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/\n",
    "    # Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    # Define the codec and create VideoWriter object.The output is stored in 'outpput.avi' file.\n",
    "    out = cv2.VideoWriter(\n",
    "        \"output.avi\",\n",
    "        cv2.VideoWriter_fourcc(\"M\", \"J\", \"P\", \"G\"),\n",
    "        4,\n",
    "        (frame_width, frame_height),\n",
    "    )\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # start_time = time.time()\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        frame = get_frame(cap)\n",
    "\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # to display the image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        frame = detect_keypoints(frame, 30)\n",
    "\n",
    "        # write to video file\n",
    "        if save_video:\n",
    "            out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        showarray(frame)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # print(\"FPS: \", 1.0 / (time.time() - start_time)) # FPS = 1 / time to process loop\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    cap.release()\n",
    "    if save_video:\n",
    "        out.release()\n",
    "    print(\"Stream stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wabcam code is based on the [displaying webcam video in IPython notebook](https://github.com/ktaletsk/NCCV/blob/master/Realtime_video_ipython.ipynb) project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
